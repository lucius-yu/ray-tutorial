{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 - Using the GPU API\n",
    "\n",
    "**GOAL:** The goal of this exercise is to show how to use GPUs with remote functions and actors.\n",
    "\n",
    "**NOTE:** These exercises are designed to run on a machine without GPUs.\n",
    "\n",
    "See the documentation on using Ray with GPUs http://ray.readthedocs.io/en/latest/using-ray-with-gpus.html.\n",
    "\n",
    "### Concepts for this Exercise - Using Ray with GPUs\n",
    "\n",
    "We can indicate that a remote function or an actor requires some GPUs using the `num_gpus` keyword.\n",
    "\n",
    "```python\n",
    "@ray.remote(num_gpus=1)\n",
    "def f():\n",
    "    # The command ray.get_gpu_ids() returns a list of the indices\n",
    "    # of the GPUs that this task can use (e.g., [0] or [1]).\n",
    "    ray.get_gpu_ids()\n",
    "\n",
    "@ray.remote(num_gpus=2)\n",
    "class Foo(object):\n",
    "    def __init__(self):\n",
    "        # The command ray.get_gpu_ids() returns a list of the\n",
    "        # indices of the GPUs that this actor can use\n",
    "        # (e.g., [0, 1] or [3, 5]).\n",
    "        ray.get_gpu_ids()\n",
    "```\n",
    "\n",
    "Then inside of the actor constructor and methods, we can get the IDs of the GPUs allocated for that actor with `ray.get_gpu_ids()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Ray, note that we pass in `num_gpus=4`. Ray will assume this machine has 4 GPUs (even if it does not). When a task or actor requests a GPU, it will be assigned a GPU ID from the set `[0, 1, 2, 3]`. It is then the responsibility of the task or actor to make sure that it only uses that specific GPU (e.g., by setting the `CUDA_VISIBLE_DEVICES` environment variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 13:09:12,650\tINFO resource_spec.py:212 -- Starting Ray with 55.66 GiB memory available for workers and up to 27.83 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-03 13:09:12,946\tWARNING services.py:1470 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 8589934592 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.68.34.74',\n",
       " 'redis_address': '10.68.34.74:58561',\n",
       " 'object_store_address': '/tmp/ray/session_2020-05-03_13-09-12_647438_1459/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-05-03_13-09-12_647438_1459/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-05-03_13-09-12_647438_1459'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=4, num_gpus=2, include_webui=False, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Change the remote function below to require one GPU.\n",
    "\n",
    "**NOTE:** This change does not make the remote function actually **use** the GPU, it simply **reserves** the GPU for use by the remote function. To actually use the GPU, the remote function would use a neural net library like TensorFlow or PyTorch after setting the `CUDA_VISIBLE_DEVICES` environment variable properly. This can be done as follows.\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in ray.get_gpu_ids()])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def f():\n",
    "    time.sleep(0.5)\n",
    "    return ray.get_gpu_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERIFY:** This code checks that each task was assigned one GPU and that not more than two tasks are run at the same time (because we told Ray there are only two GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [0]]\n",
      "Sucess! The test passed.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "gpu_ids = ray.get([f.remote() for _ in range(3)])\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(gpu_ids)\n",
    "\n",
    "for i in range(len(gpu_ids)):\n",
    "    assert len(gpu_ids[i]) == 1\n",
    "\n",
    "assert end_time - start_time > 1\n",
    "\n",
    "print('Sucess! The test passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1588510814.6945872, 1588510814.17879, 0.5157971382141113)\n"
     ]
    }
   ],
   "source": [
    "print((end_time, start_time, end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** The code below defines an actor. Make it require one GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "class Actor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_gpu_ids(self):\n",
    "        return ray.get_gpu_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERIFY:** This code checks that the actor was assigned a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess! The test passed.\n"
     ]
    }
   ],
   "source": [
    "actor = Actor.remote()\n",
    "\n",
    "gpu_ids = ray.get(actor.get_gpu_ids.remote())\n",
    "\n",
    "assert len(gpu_ids) == 1\n",
    "\n",
    "print('Sucess! The test passed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
